{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In this chapter we build GPT tokenizer",
   "id": "1b0688dd10957157"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "byte-pair encoding is pretty simple:\n",
    "* we determine a most popular pair of bytes\n",
    "* we invent a new byte and replace the pair with the new byte\n",
    "* repeat"
   ],
   "id": "751121ad7b2de009"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-03T10:17:39.571395Z",
     "start_time": "2025-08-03T10:17:39.564010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import collections\n",
    "import itertools\n",
    "\n",
    "Pair = tuple[int, int]\n",
    "Merges = dict[Pair, int]\n",
    "\n",
    "\n",
    "def build_merges(text: str, num_iterations: int) -> Merges:\n",
    "    merges = {}\n",
    "    encoded = list(text.encode('utf-8'))\n",
    "    new_bytes = itertools.count(256)\n",
    "    for _ in range(num_iterations):\n",
    "        if len(encoded) < 2:\n",
    "            break\n",
    "        pair = find_most_popular_pair(encoded)\n",
    "        replacement = next(new_bytes)\n",
    "        encoded = replace(encoded, pair, replacement)\n",
    "        merges[pair] = replacement\n",
    "    return merges\n",
    "\n",
    "\n",
    "def find_most_popular_pair(encoded: list[int]) -> tuple[int, int]:\n",
    "    counts = collections.Counter()\n",
    "    prev = None\n",
    "    for cur in encoded:\n",
    "        if prev is not None:\n",
    "            pair = (prev, cur)\n",
    "            counts[pair] += 1\n",
    "        prev = cur\n",
    "    return counts.most_common(1)[0][0]\n",
    "\n",
    "\n",
    "def replace(encoded: list[int], what: tuple[int, int], replacement: int):\n",
    "    result = []\n",
    "    state = []\n",
    "    for cur in encoded:\n",
    "        # invariant: len(state) < 2\n",
    "        state.append(cur)\n",
    "        if tuple(state) == what:\n",
    "            result.append(replacement)\n",
    "            state = []\n",
    "        elif len(state) == 2:\n",
    "            # not a match, we can add the first element, since it's not part of the `what` pair\n",
    "            result.append(state[0])\n",
    "            state.pop(0)\n",
    "\n",
    "    # invariant: len(state) < 2\n",
    "    result.extend(state)\n",
    "    return result"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T10:18:39.733193Z",
     "start_time": "2025-08-03T10:18:39.730476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "ascii_text = \"aaabdaaabac\"\n",
    "ascii_merges = build_merges(ascii_text, num_iterations=3)"
   ],
   "id": "e318b8c06151337b",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T10:18:37.811176Z",
     "start_time": "2025-08-03T10:18:37.808197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "russian_text = \"приветики вам, хочу проверить byte-pair encoding\"\n",
    "russian_merges = build_merges(russian_text, num_iterations=20)"
   ],
   "id": "32320570491459d9",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T17:41:32.175080Z",
     "start_time": "2025-08-04T17:41:32.171937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def byte_pair_encode(text: str, merges: Merges) -> list[int]:\n",
    "    \"\"\"\n",
    "    we need to replace each pair in the same order\n",
    "    merges are in topological order\n",
    "    same merge can't be applied twice\n",
    "    this is probably not the most efficient way to do it\n",
    "    karpathy's implementation is faster\n",
    "    \"\"\"\n",
    "    encoded = list(text.encode(\"utf-8\"))\n",
    "    for (a, b), m in merges.items():\n",
    "        encoded = replace(encoded, (a, b), m)\n",
    "    return encoded\n",
    "\n",
    "\n",
    "def byte_pair_decode(encoded: list[int], merges: Merges) -> str:\n",
    "    # all simple tokens are in topological order (they don't depend on each other)\n",
    "    # all composite tokens are in topological order:\n",
    "    # dictionary is ordered\n",
    "    # and each new token depends on already defined tokens\n",
    "    # already defined token can appear as a new value in a merges\n",
    "    token_values = {i: [i] for i in range(256)}\n",
    "    for (a, b), m in merges.items():\n",
    "        token_values[m] = token_values[a] + token_values[b]\n",
    "    decoded = []\n",
    "    for token in encoded:\n",
    "        decoded.extend(token_values[token])\n",
    "    return bytes(decoded).decode('utf-8')"
   ],
   "id": "cb60ceaa11890ab4",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T17:41:38.019549Z",
     "start_time": "2025-08-04T17:41:38.016440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ascii_encoded = byte_pair_encode(ascii_text, merges=ascii_merges)\n",
    "print(byte_pair_decode(ascii_encoded, ascii_merges))"
   ],
   "id": "be6a1d8825c1bd7c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaabdaaabac\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T17:41:41.199819Z",
     "start_time": "2025-08-04T17:41:41.195959Z"
    }
   },
   "cell_type": "code",
   "source": [
    "russian_encoded = byte_pair_encode(russian_text, merges=russian_merges)\n",
    "print(byte_pair_decode(russian_encoded, russian_merges))"
   ],
   "id": "1430dcb569157407",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "приветики вам, хочу проверить byte-pair encoding\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Real implementation of byte pair encoding/decoding in OpenAI GPT are doing extra processing: they split text into different categories of characters (letters, numbers, punctuation) and bpe can't cross this boundaries during merges.\n",
    "\n",
    "The reason for that is that it's not right to mix punctuation & letters:\n",
    "let's say you have a separate tokens for \"dog\", \"dog.\", \"dog?\". It's the same concept but it will be represented by different tokens, which can't help."
   ],
   "id": "7298ab688873bdf2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Limitations\n",
    "Some of the limitations of LLMs are due to tokenization. Token is not a character, it's a sequence of characters. That's why it's hard for LLMs to do character-level manipulations (like count number of characters, reverse a string, etc). Same with arithmetic: e.g 4 digit number can be any of the combinations of tokens (one token of length 4; or one token of length 3 and one token of length 1; etc), so it's a miracle LLM can do arithmetic at all."
   ],
   "id": "8ee7930a64549af"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Tiktoken\n",
    "Tiktoken is a library from OpenAI for tokenization, it can encode/decode text <-> tokens"
   ],
   "id": "1359ceca7d459f35"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T17:47:57.988854Z",
     "start_time": "2025-08-04T17:47:48.180619Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"o200k_base\")"
   ],
   "id": "c582211b288fff2a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aershov/Projects/nn-zero-to-hero/venv/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T17:48:47.429380Z",
     "start_time": "2025-08-04T17:48:47.422989Z"
    }
   },
   "cell_type": "code",
   "source": "encoding.encode(\"some text SolidGoldMagikarp\")",
   "id": "25b7724d06c834d4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25231, 2201, 35764, 30717, 20101, 507, 11784]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As you can see text was pretty long, but was tokenized just to 7 tokens.\n",
    "We can decode it back."
   ],
   "id": "50b964136bb50694"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T17:49:31.778370Z",
     "start_time": "2025-08-04T17:49:31.774926Z"
    }
   },
   "cell_type": "code",
   "source": "encoding.decode(encoding.encode(\"some text SolidGoldMagikarp\"))",
   "id": "4381e08e0dfbc18f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'some text SolidGoldMagikarp'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "tiktoken provides only inference code - you can't train tokenizer with it. It's just a tokenizer used in OpenAI models.",
   "id": "914528db32d5ee57"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Sentencepiece",
   "id": "c740a7f9a33ba34f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "sentencepiece library also provides tokenizer with training capabilities.",
   "id": "844860ba7b92b1cd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T17:52:20.737974Z",
     "start_time": "2025-08-04T17:52:20.731928Z"
    }
   },
   "cell_type": "code",
   "source": "import sentencepiece as spm",
   "id": "87e81c4882a4d5cc",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T17:56:18.390366Z",
     "start_time": "2025-08-04T17:56:18.378143Z"
    }
   },
   "cell_type": "code",
   "source": "spm.SentencePieceTrainer.train(sentence_iterator=iter([\"Call me Ishmael\", \"Some years ago—never mind how long precisely—having little or no money in my purse, and nothing particular to interest me on shore, I thought I would sail about a little and see the watery part of the world\"]), model_prefix = 'm', vocab_size = 30)",
   "id": "ad0a80c7e147f98f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input_format: \n",
      "  model_prefix: m\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 30\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 2 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=223\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=27\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 2 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=95\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 63 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 2\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 36\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 36 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=56 obj=13.8944 num_tokens=130 num_tokens/piece=2.32143\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=56 obj=13.855 num_tokens=130 num_tokens/piece=2.32143\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=42 obj=15.653 num_tokens=144 num_tokens/piece=3.42857\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=42 obj=15.0844 num_tokens=144 num_tokens/piece=3.42857\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=33 obj=19.44 num_tokens=165 num_tokens/piece=5\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=33 obj=18.8757 num_tokens=165 num_tokens/piece=5\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: m.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: m.vocab\n"
     ]
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T17:57:02.059362Z",
     "start_time": "2025-08-04T17:57:02.054766Z"
    }
   },
   "cell_type": "code",
   "source": "sp = spm.SentencePieceProcessor(model_file=\"m.model\")",
   "id": "b5368e5c840d2de2",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T17:57:47.460188Z",
     "start_time": "2025-08-04T17:57:47.455855Z"
    }
   },
   "cell_type": "code",
   "source": "sp.encode(\"howdy\")",
   "id": "1c3381e6e597bc67",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 10, 8, 22, 12, 14]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "That was very weird result (tokenization is longer than initial text), but looks like sentencepiece is pretty messy.",
   "id": "13e3e2f3e1e2e402"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
