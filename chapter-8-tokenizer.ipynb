{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In this chapter we build GPT tokenizer",
   "id": "1b0688dd10957157"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "byte-pair encoding is pretty simple:\n",
    "* we determine a most popular pair of bytes\n",
    "* we invent a new byte and replace the pair with the new byte\n",
    "* repeat"
   ],
   "id": "751121ad7b2de009"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-02T10:46:38.874321Z",
     "start_time": "2025-08-02T10:46:38.868258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import collections\n",
    "import itertools\n",
    "\n",
    "\n",
    "Pair = tuple[int, int]\n",
    "Merges = dict[Pair, int]\n",
    "\n",
    "# TODO: rename, so it won't conflict with byte_pair_encode that treats merges as\n",
    "# an input not as output\n",
    "def byte_pair_encode(text: str, num_iterations: int, merges: Merges) -> list[int]:\n",
    "    encoded = list(text.encode('utf-8'))\n",
    "    new_bytes = itertools.count(256)\n",
    "    for _ in range(num_iterations):\n",
    "        if len(encoded) < 2:\n",
    "            break\n",
    "        pair = find_most_popular_pair(encoded)\n",
    "        replacement = next(new_bytes)\n",
    "        encoded = replace(encoded, pair, replacement)\n",
    "        merges[pair] = replacement\n",
    "    return encoded\n",
    "\n",
    "\n",
    "def find_most_popular_pair(encoded: list[int]) -> tuple[int, int]:\n",
    "    counts = collections.Counter()\n",
    "    prev = None\n",
    "    for cur in encoded:\n",
    "        if prev is not None:\n",
    "            pair = (prev, cur)\n",
    "            counts[pair] += 1\n",
    "        prev = cur\n",
    "    return counts.most_common(1)[0][0]\n",
    "\n",
    "\n",
    "def replace(encoded: list[int], what: tuple[int, int], replacement: int):\n",
    "    result = []\n",
    "    state = []\n",
    "    for cur in encoded:\n",
    "        # invariant: len(state) < 2\n",
    "        state.append(cur)\n",
    "        if tuple(state) == what:\n",
    "            result.append(replacement)\n",
    "            state = []\n",
    "        elif len(state) == 2:\n",
    "            # not a match, we can add the first element, since it's not part of the `what` pair\n",
    "            result.append(state[0])\n",
    "            state.pop(0)\n",
    "\n",
    "    # invariant: len(state) < 2\n",
    "    result.extend(state)\n",
    "    return result"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T10:19:42.742574Z",
     "start_time": "2025-08-02T10:19:42.738154Z"
    }
   },
   "cell_type": "code",
   "source": "byte_pair_encode(\"aaabdaaabac\", num_iterations=3)",
   "id": "e318b8c06151337b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[258, 100, 258, 97, 99]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T10:18:28.498491Z",
     "start_time": "2025-08-02T10:18:28.493346Z"
    }
   },
   "cell_type": "code",
   "source": "len(byte_pair_encode(\"приветики вам, хочу проверить byte-pair encoding\", num_iterations=20))",
   "id": "32320570491459d9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T10:54:18.704697Z",
     "start_time": "2025-08-02T10:54:18.701165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def byte_pair_decode(encoded: list[int], merges: Merges) -> str:\n",
    "    # TODO: optimize, use toposort or smth similar to make it more efficient\n",
    "    token_values = {m: [a, b] for (a, b), m in merges.items()}\n",
    "\n",
    "    decoded_parts = encoded\n",
    "    replaced = True\n",
    "    while replaced:\n",
    "        replaced = False\n",
    "\n",
    "        new_decoded_parts = []\n",
    "        for token in decoded_parts:\n",
    "            if token in token_values:\n",
    "                replaced = True\n",
    "                new_decoded_parts.extend(token_values[token])\n",
    "            else:\n",
    "                new_decoded_parts.append(token)\n",
    "        decoded_parts = new_decoded_parts\n",
    "    return bytes(decoded_parts).decode(\"utf-8\")\n",
    "\n",
    "# TODO: implement byte_pair_encode that is symmetric to byte_pair_decode:\n",
    "# it should take merges and don't modify them"
   ],
   "id": "cb60ceaa11890ab4",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T10:54:20.458868Z",
     "start_time": "2025-08-02T10:54:20.455249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ascii_merges = {}\n",
    "ascii_encoded = byte_pair_encode(\"aaabdaaabac\", num_iterations=3, merges=ascii_merges)\n",
    "\n",
    "print(byte_pair_decode(ascii_encoded, ascii_merges))"
   ],
   "id": "be6a1d8825c1bd7c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaabdaaabac\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Real implementation of byte pair encoding/decoding in OpenAI GPT are doing extra processing: they split text into different categories of characters (letters, numbers, punctuation) and bpe can't cross this boundaries during merges.\n",
    "\n",
    "The reason for that is that it's not right to mix punctuation & letters:\n",
    "let's say you have a separate tokens for \"dog\", \"dog.\", \"dog?\". It's the same concept but it will be represented by different tokens, which can't help."
   ],
   "id": "7298ab688873bdf2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
