{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In this chapter we build GPT tokenizer",
   "id": "1b0688dd10957157"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "byte-pair encoding is pretty simple:\n",
    "* we determine a most popular pair of bytes\n",
    "* we invent a new byte and replace the pair with the new byte\n",
    "* repeat"
   ],
   "id": "751121ad7b2de009"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-03T10:17:39.571395Z",
     "start_time": "2025-08-03T10:17:39.564010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import collections\n",
    "import itertools\n",
    "\n",
    "Pair = tuple[int, int]\n",
    "Merges = dict[Pair, int]\n",
    "\n",
    "\n",
    "def build_merges(text: str, num_iterations: int) -> Merges:\n",
    "    merges = {}\n",
    "    encoded = list(text.encode('utf-8'))\n",
    "    new_bytes = itertools.count(256)\n",
    "    for _ in range(num_iterations):\n",
    "        if len(encoded) < 2:\n",
    "            break\n",
    "        pair = find_most_popular_pair(encoded)\n",
    "        replacement = next(new_bytes)\n",
    "        encoded = replace(encoded, pair, replacement)\n",
    "        merges[pair] = replacement\n",
    "    return merges\n",
    "\n",
    "\n",
    "def find_most_popular_pair(encoded: list[int]) -> tuple[int, int]:\n",
    "    counts = collections.Counter()\n",
    "    prev = None\n",
    "    for cur in encoded:\n",
    "        if prev is not None:\n",
    "            pair = (prev, cur)\n",
    "            counts[pair] += 1\n",
    "        prev = cur\n",
    "    return counts.most_common(1)[0][0]\n",
    "\n",
    "\n",
    "def replace(encoded: list[int], what: tuple[int, int], replacement: int):\n",
    "    result = []\n",
    "    state = []\n",
    "    for cur in encoded:\n",
    "        # invariant: len(state) < 2\n",
    "        state.append(cur)\n",
    "        if tuple(state) == what:\n",
    "            result.append(replacement)\n",
    "            state = []\n",
    "        elif len(state) == 2:\n",
    "            # not a match, we can add the first element, since it's not part of the `what` pair\n",
    "            result.append(state[0])\n",
    "            state.pop(0)\n",
    "\n",
    "    # invariant: len(state) < 2\n",
    "    result.extend(state)\n",
    "    return result"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T10:18:39.733193Z",
     "start_time": "2025-08-03T10:18:39.730476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "ascii_text = \"aaabdaaabac\"\n",
    "ascii_merges = build_merges(ascii_text, num_iterations=3)"
   ],
   "id": "e318b8c06151337b",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T10:18:37.811176Z",
     "start_time": "2025-08-03T10:18:37.808197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "russian_text = \"приветики вам, хочу проверить byte-pair encoding\"\n",
    "russian_merges = build_merges(russian_text, num_iterations=20)"
   ],
   "id": "32320570491459d9",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T17:41:32.175080Z",
     "start_time": "2025-08-04T17:41:32.171937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def byte_pair_encode(text: str, merges: Merges) -> list[int]:\n",
    "    \"\"\"\n",
    "    we need to replace each pair in the same order\n",
    "    TODO: is this implementation correct?\n",
    "    \"\"\"\n",
    "    encoded = list(text.encode(\"utf-8\"))\n",
    "    for (a, b), m in merges.items():\n",
    "        encoded = replace(encoded, (a, b), m)\n",
    "    return encoded\n",
    "\n",
    "\n",
    "def byte_pair_decode(encoded: list[int], merges: Merges) -> str:\n",
    "    # all simple tokens are in topological order (they don't depend on each other)\n",
    "    # all composite tokens are in topological order:\n",
    "    # dictionary is ordered\n",
    "    # and each new token depends on already defined tokens\n",
    "    # already defined token can appear as a new value in a merges\n",
    "    token_values = {i: [i] for i in range(256)}\n",
    "    for (a, b), m in merges.items():\n",
    "        token_values[m] = token_values[a] + token_values[b]\n",
    "    decoded = []\n",
    "    for token in encoded:\n",
    "        decoded.extend(token_values[token])\n",
    "    return bytes(decoded).decode('utf-8')"
   ],
   "id": "cb60ceaa11890ab4",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T17:41:38.019549Z",
     "start_time": "2025-08-04T17:41:38.016440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ascii_encoded = byte_pair_encode(ascii_text, merges=ascii_merges)\n",
    "print(byte_pair_decode(ascii_encoded, ascii_merges))"
   ],
   "id": "be6a1d8825c1bd7c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaabdaaabac\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T17:41:41.199819Z",
     "start_time": "2025-08-04T17:41:41.195959Z"
    }
   },
   "cell_type": "code",
   "source": [
    "russian_encoded = byte_pair_encode(russian_text, merges=russian_merges)\n",
    "print(byte_pair_decode(russian_encoded, russian_merges))"
   ],
   "id": "1430dcb569157407",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "приветики вам, хочу проверить byte-pair encoding\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Real implementation of byte pair encoding/decoding in OpenAI GPT are doing extra processing: they split text into different categories of characters (letters, numbers, punctuation) and bpe can't cross this boundaries during merges.\n",
    "\n",
    "The reason for that is that it's not right to mix punctuation & letters:\n",
    "let's say you have a separate tokens for \"dog\", \"dog.\", \"dog?\". It's the same concept but it will be represented by different tokens, which can't help."
   ],
   "id": "7298ab688873bdf2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "TODO: add tiktoken, sentencepiece.",
   "id": "af2a74a557e1f72b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Limitations\n",
    "Some of the limitations of LLMs are due to tokenization. Token is not a character, it's a sequence of characters. That's why it's hard for LLMs to do character-level manipulations (like count number of characters, reverse a string, etc). Same with arithmetic: e.g 4 digit number can be any of the combinations of tokens (one token of length 4; or one token of length 3 and one token of length 1; etc), so it's a miracle LLM can do arithmetic at all."
   ],
   "id": "8ee7930a64549af"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Tiktoken\n",
    "Tiktoken is a library from OpenAI for tokenization, it can encode/decode text <-> tokens"
   ],
   "id": "1359ceca7d459f35"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T17:47:57.988854Z",
     "start_time": "2025-08-04T17:47:48.180619Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"o200k_base\")"
   ],
   "id": "c582211b288fff2a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aershov/Projects/nn-zero-to-hero/venv/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T17:48:47.429380Z",
     "start_time": "2025-08-04T17:48:47.422989Z"
    }
   },
   "cell_type": "code",
   "source": "encoding.encode(\"some text SolidGoldMagikarp\")",
   "id": "25b7724d06c834d4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25231, 2201, 35764, 30717, 20101, 507, 11784]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As you can see text was pretty long, but was tokenized just to 7 tokens.\n",
    "We can decode it back."
   ],
   "id": "50b964136bb50694"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T17:49:31.778370Z",
     "start_time": "2025-08-04T17:49:31.774926Z"
    }
   },
   "cell_type": "code",
   "source": "encoding.decode(encoding.encode(\"some text SolidGoldMagikarp\"))",
   "id": "4381e08e0dfbc18f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'some text SolidGoldMagikarp'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "tiktoken provides only inference code - you can't train tokenizer with it. It's just a tokenizer used in OpenAI models.",
   "id": "914528db32d5ee57"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
