{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "690f5ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91edd01a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32033, ['emma', 'olivia', 'ava'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_names(path):\n",
    "    with open(path) as fileobj:\n",
    "        names = [line.strip() for line in fileobj]\n",
    "    return names\n",
    "\n",
    "NAMES = read_names('names.txt')\n",
    "len(NAMES), NAMES[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ad15ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': 0,\n",
       " 'a': 1,\n",
       " 'b': 2,\n",
       " 'c': 3,\n",
       " 'd': 4,\n",
       " 'e': 5,\n",
       " 'f': 6,\n",
       " 'g': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'j': 10,\n",
       " 'k': 11,\n",
       " 'l': 12,\n",
       " 'm': 13,\n",
       " 'n': 14,\n",
       " 'o': 15,\n",
       " 'p': 16,\n",
       " 'q': 17,\n",
       " 'r': 18,\n",
       " 's': 19,\n",
       " 't': 20,\n",
       " 'u': 21,\n",
       " 'v': 22,\n",
       " 'w': 23,\n",
       " 'x': 24,\n",
       " 'y': 25,\n",
       " 'z': 26}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "CHAR_TO_CLASS = {char: i for i, char in enumerate('.' + string.ascii_lowercase)}\n",
    "CLASS_TO_CHAR = {i: char for char, i in CHAR_TO_CLASS.items()}\n",
    "\n",
    "CHAR_TO_CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a444430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(228146,\n",
       " [('...', 'e'),\n",
       "  ('..e', 'm'),\n",
       "  ('.em', 'm'),\n",
       "  ('emm', 'a'),\n",
       "  ('mma', '.'),\n",
       "  ('...', 'o')])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONTEXT_LEN = 3\n",
    "\n",
    "def build_training_data(names, context_len):\n",
    "    training_data = []\n",
    "    prefix = '.' * context_len\n",
    "    for a_name in names:\n",
    "        a_name += '.'\n",
    "        full_name = f'{prefix}{a_name}'\n",
    "        for i, next_char in enumerate(a_name):\n",
    "            context = full_name[i:i + context_len]\n",
    "            training_data.append((context, next_char))\n",
    "    return training_data\n",
    "\n",
    "TRAINING_DATA = build_training_data(NAMES, CONTEXT_LEN)\n",
    "len(TRAINING_DATA), TRAINING_DATA[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "353af74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUTS.shape=torch.Size([228146, 3]),  INPUTS[:3]=tensor([[ 0,  0,  0],\n",
      "        [ 0,  0,  5],\n",
      "        [ 0,  5, 13]]), OUTPUTS.shape=torch.Size([228146]), OUTPUTS[:3]=tensor([ 5, 13, 13])\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = len(CLASS_TO_CHAR)\n",
    "\n",
    "def build_training_set(training_set):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    for context, next_char in training_set:\n",
    "        an_input = [CHAR_TO_CLASS[char] for char in context]\n",
    "        inputs.append(an_input)\n",
    "        \n",
    "        an_output = CHAR_TO_CLASS[next_char]\n",
    "        outputs.append(an_output)\n",
    "    inputs = torch.tensor(inputs)\n",
    "    outputs = torch.tensor(outputs)\n",
    "    return inputs, outputs\n",
    "\n",
    "\n",
    "INPUTS, OUTPUTS = build_training_set(TRAINING_DATA)\n",
    "print(f'{INPUTS.shape=},  {INPUTS[:3]=}, {OUTPUTS.shape=}, {OUTPUTS[:3]=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "735a02ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time.time()=1691235498.37 i=0 loss.data=tensor(8.4536)\n",
      "time.time()=1691235500.05 i=10000 loss.data=tensor(4.0960)\n",
      "time.time()=1691235501.67 i=20000 loss.data=tensor(3.0769)\n",
      "time.time()=1691235503.26 i=30000 loss.data=tensor(2.7793)\n",
      "time.time()=1691235504.84 i=40000 loss.data=tensor(2.5716)\n",
      "time.time()=1691235506.43 i=50000 loss.data=tensor(2.5750)\n",
      "time.time()=1691235508.03 i=60000 loss.data=tensor(2.5696)\n",
      "time.time()=1691235509.62 i=70000 loss.data=tensor(1.9596)\n",
      "time.time()=1691235511.19 i=80000 loss.data=tensor(2.5666)\n",
      "time.time()=1691235512.75 i=90000 loss.data=tensor(2.3063)\n",
      "time.time()=1691235514.34 i=100000 loss.data=tensor(2.4602)\n",
      "time.time()=1691235515.94 i=110000 loss.data=tensor(2.1502)\n",
      "time.time()=1691235517.52 i=120000 loss.data=tensor(2.1348)\n",
      "time.time()=1691235519.12 i=130000 loss.data=tensor(2.0987)\n",
      "time.time()=1691235520.74 i=140000 loss.data=tensor(2.2463)\n",
      "time.time()=1691235522.30 i=150000 loss.data=tensor(2.0326)\n",
      "time.time()=1691235523.87 i=160000 loss.data=tensor(2.1219)\n",
      "time.time()=1691235525.43 i=170000 loss.data=tensor(2.0257)\n",
      "time.time()=1691235526.96 i=180000 loss.data=tensor(2.0897)\n",
      "time.time()=1691235528.55 i=190000 loss.data=tensor(2.0012)\n",
      "loss=tensor(2.0518, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def requires_grad(t):\n",
    "    t.requires_grad = True\n",
    "    return t\n",
    "\n",
    "C_DIM = 10\n",
    "    \n",
    "\n",
    "def train(inputs, outputs, num_iterations=100_000, verbose=False):\n",
    "    batch_size = 32\n",
    "    \n",
    "    assert len(inputs) == len(outputs)\n",
    "    \n",
    "    hidden_out_size = 200\n",
    "    # we place each character into C_DIM dimensional vector\n",
    "    C = requires_grad(torch.randn(NUM_CLASSES, C_DIM))\n",
    "    # input of hidden layer is CONTEXT_LEN * C_DIM elements\n",
    "    hidden_w = requires_grad(torch.randn(CONTEXT_LEN * C_DIM, hidden_out_size))\n",
    "    hidden_b = requires_grad(torch.randn(hidden_out_size))\n",
    "    # input of final layer is hidden_out_size\n",
    "    last_w = requires_grad(torch.randn(hidden_out_size, NUM_CLASSES))\n",
    "    last_b = requires_grad(torch.randn(NUM_CLASSES))\n",
    "    # output of the network is 27\n",
    "    parameters = [C, hidden_w, hidden_b, last_w, last_b]\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        batch_indexes = torch.randint(0, inputs.shape[0], (batch_size,))\n",
    "        # inputs_batch.shape = (batch_size, CONTEXT_LEN)\n",
    "        inputs_batch = inputs[batch_indexes]\n",
    "        outputs_batch = outputs[batch_indexes]\n",
    "        probs = forward(inputs_batch, parameters)\n",
    "        m_probs = probs[torch.arange(len(probs)), outputs_batch]\n",
    "        loss = -(m_probs.log().mean())\n",
    "\n",
    "        # loss = F.cross_entropy(probs, outputs_batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        # learning_rate decay\n",
    "        learning_rate = 1e-1 if i < 100_000 else 1e-2\n",
    "        if verbose and i % 10000 == 0:\n",
    "            print(f'{time.time()=:.2f} {i=} {loss.data=}')\n",
    "        for p in parameters:\n",
    "            p.data -= learning_rate * p.grad\n",
    "            p.grad = None\n",
    "    print(f'{loss=}')\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def forward(inputs_batch, parameters):\n",
    "    smooth = 0.0001\n",
    "    C, hidden_w, hidden_b, last_w, last_b = parameters\n",
    "    # C[inputs_batch].shape = (batch_size, CONTEXT_LEN, C_DIM)\n",
    "    # first_output.shape == (batch_size, CONTEXT_LEN * C_DIM)\n",
    "    first_output = C[inputs_batch].view(-1, C_DIM * CONTEXT_LEN)\n",
    "    hidden_output = (first_output @ hidden_w + hidden_b).tanh()\n",
    "    logits = hidden_output @ last_w + last_b\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdim=True) + smooth\n",
    "    return probs\n",
    "\n",
    "\n",
    "TRAINING_SET_SIZE = 200_000\n",
    "PARAMETERS = train(\n",
    "    INPUTS[:TRAINING_SET_SIZE], \n",
    "    OUTPUTS[:TRAINING_SET_SIZE],\n",
    "    num_iterations=200_000, \n",
    "    verbose=True\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57a18b15-31bc-45e1-b9f6-1d3d64150537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kasiya\n",
      "olura\n",
      "aalyn\n",
      "jailean\n",
      "tasiyah\n"
     ]
    }
   ],
   "source": [
    "def generate_examples(parameters):\n",
    "    num_examples = 5\n",
    "    for _ in range(num_examples):\n",
    "        \n",
    "        context = '.' * CONTEXT_LEN\n",
    "        example = ''\n",
    "        while True:\n",
    "            cur_inputs = torch.tensor([[CHAR_TO_CLASS[char] for char in context]])\n",
    "            probs = forward(cur_inputs, parameters)\n",
    "            next_class = torch.multinomial(probs, 1).item()\n",
    "            next_char = CLASS_TO_CHAR[next_class]\n",
    "            if next_char == '.':\n",
    "                break\n",
    "            example += next_char\n",
    "            context = context[1:] + next_char\n",
    "        print(example)\n",
    "\n",
    "generate_examples(PARAMETERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f1603d-5571-4788-aa59-174e62df536a",
   "metadata": {},
   "source": [
    "These examples are better than those made with the bigram model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
