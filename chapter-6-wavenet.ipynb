{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "283487ad-1c89-4c63-b7ea-a44b6f580202",
   "metadata": {},
   "source": [
    "In this chapter we build a nn with several layers. We group each layer by two characters and pass it through to the next layer etc.\n",
    "This allows neural network to learn more intricate relationship between characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "690f5ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91edd01a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32033, ['emma', 'olivia', 'ava'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_names(path):\n",
    "    with open(path) as fileobj:\n",
    "        names = [line.strip() for line in fileobj]\n",
    "    return names\n",
    "\n",
    "NAMES = read_names('names.txt')\n",
    "len(NAMES), NAMES[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ad15ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': 0,\n",
       " 'a': 1,\n",
       " 'b': 2,\n",
       " 'c': 3,\n",
       " 'd': 4,\n",
       " 'e': 5,\n",
       " 'f': 6,\n",
       " 'g': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'j': 10,\n",
       " 'k': 11,\n",
       " 'l': 12,\n",
       " 'm': 13,\n",
       " 'n': 14,\n",
       " 'o': 15,\n",
       " 'p': 16,\n",
       " 'q': 17,\n",
       " 'r': 18,\n",
       " 's': 19,\n",
       " 't': 20,\n",
       " 'u': 21,\n",
       " 'v': 22,\n",
       " 'w': 23,\n",
       " 'x': 24,\n",
       " 'y': 25,\n",
       " 'z': 26}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "CHAR_TO_CLASS = {char: i for i, char in enumerate('.' + string.ascii_lowercase)}\n",
    "CLASS_TO_CHAR = {i: char for char, i in CHAR_TO_CLASS.items()}\n",
    "\n",
    "CHAR_TO_CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a444430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(228146,\n",
       " [('........', 'e'),\n",
       "  ('.......e', 'm'),\n",
       "  ('......em', 'm'),\n",
       "  ('.....emm', 'a'),\n",
       "  ('....emma', '.'),\n",
       "  ('........', 'o')])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONTEXT_LEN = 8\n",
    "\n",
    "def build_training_data(names, context_len):\n",
    "    training_data = []\n",
    "    prefix = '.' * context_len\n",
    "    for a_name in names:\n",
    "        a_name += '.'\n",
    "        full_name = f'{prefix}{a_name}'\n",
    "        for i, next_char in enumerate(a_name):\n",
    "            context = full_name[i:i + context_len]\n",
    "            training_data.append((context, next_char))\n",
    "    return training_data\n",
    "\n",
    "TRAINING_DATA = build_training_data(NAMES, CONTEXT_LEN)\n",
    "len(TRAINING_DATA), TRAINING_DATA[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "353af74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUTS.shape=torch.Size([228146, 8]),  INPUTS[:3]=tensor([[ 0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  5],\n",
      "        [ 0,  0,  0,  0,  0,  0,  5, 13]]), OUTPUTS.shape=torch.Size([228146]), OUTPUTS[:3]=tensor([ 5, 13, 13])\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = len(CLASS_TO_CHAR)\n",
    "\n",
    "def build_training_set(training_set):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    for context, next_char in training_set:\n",
    "        an_input = [CHAR_TO_CLASS[char] for char in context]\n",
    "        inputs.append(an_input)\n",
    "        \n",
    "        an_output = CHAR_TO_CLASS[next_char]\n",
    "        outputs.append(an_output)\n",
    "    inputs = torch.tensor(inputs)\n",
    "    outputs = torch.tensor(outputs)\n",
    "    return inputs, outputs\n",
    "\n",
    "\n",
    "INPUTS, OUTPUTS = build_training_set(TRAINING_DATA)\n",
    "print(f'{INPUTS.shape=},  {INPUTS[:3]=}, {OUTPUTS.shape=}, {OUTPUTS[:3]=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "735a02ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time.time()=1692692658.86 i=0 loss.data=tensor(8.8331)\n",
      "time.time()=1692692662.58 i=10000 loss.data=tensor(2.7259)\n",
      "time.time()=1692692666.34 i=20000 loss.data=tensor(2.5605)\n",
      "time.time()=1692692669.97 i=30000 loss.data=tensor(2.9242)\n",
      "time.time()=1692692673.56 i=40000 loss.data=tensor(2.4640)\n",
      "time.time()=1692692677.15 i=50000 loss.data=tensor(2.6430)\n",
      "time.time()=1692692680.70 i=60000 loss.data=tensor(2.5088)\n",
      "time.time()=1692692684.26 i=70000 loss.data=tensor(2.0211)\n",
      "time.time()=1692692687.86 i=80000 loss.data=tensor(2.1376)\n",
      "time.time()=1692692691.39 i=90000 loss.data=tensor(2.8058)\n",
      "time.time()=1692692694.94 i=100000 loss.data=tensor(2.3467)\n",
      "time.time()=1692692698.55 i=110000 loss.data=tensor(2.1369)\n",
      "time.time()=1692692702.16 i=120000 loss.data=tensor(2.1413)\n",
      "time.time()=1692692705.68 i=130000 loss.data=tensor(2.1387)\n",
      "time.time()=1692692709.25 i=140000 loss.data=tensor(2.4025)\n",
      "time.time()=1692692712.79 i=150000 loss.data=tensor(2.1486)\n",
      "time.time()=1692692716.31 i=160000 loss.data=tensor(2.1050)\n",
      "time.time()=1692692719.82 i=170000 loss.data=tensor(2.0057)\n",
      "time.time()=1692692723.38 i=180000 loss.data=tensor(2.4945)\n",
      "time.time()=1692692726.88 i=190000 loss.data=tensor(2.3979)\n",
      "time.time()=1692692730.38 i=200000 loss.data=tensor(2.1489)\n",
      "time.time()=1692692733.93 i=210000 loss.data=tensor(2.6124)\n",
      "time.time()=1692692737.45 i=220000 loss.data=tensor(2.4170)\n",
      "time.time()=1692692740.91 i=230000 loss.data=tensor(2.3333)\n",
      "time.time()=1692692744.46 i=240000 loss.data=tensor(2.2746)\n",
      "time.time()=1692692748.01 i=250000 loss.data=tensor(2.1127)\n",
      "time.time()=1692692751.54 i=260000 loss.data=tensor(2.5451)\n",
      "time.time()=1692692755.08 i=270000 loss.data=tensor(2.4264)\n",
      "time.time()=1692692758.67 i=280000 loss.data=tensor(2.2994)\n",
      "time.time()=1692692762.19 i=290000 loss.data=tensor(2.3044)\n",
      "time.time()=1692692765.73 i=300000 loss.data=tensor(2.2177)\n",
      "time.time()=1692692769.33 i=310000 loss.data=tensor(2.5759)\n",
      "time.time()=1692692772.88 i=320000 loss.data=tensor(1.9132)\n",
      "time.time()=1692692776.40 i=330000 loss.data=tensor(2.1292)\n",
      "time.time()=1692692780.00 i=340000 loss.data=tensor(2.0234)\n",
      "time.time()=1692692783.56 i=350000 loss.data=tensor(2.5231)\n",
      "time.time()=1692692787.08 i=360000 loss.data=tensor(2.5081)\n",
      "time.time()=1692692790.65 i=370000 loss.data=tensor(1.6169)\n",
      "time.time()=1692692794.25 i=380000 loss.data=tensor(1.9178)\n",
      "time.time()=1692692797.77 i=390000 loss.data=tensor(2.0529)\n",
      "loss=tensor(2.7446, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def requires_grad(t):\n",
    "    t.requires_grad = True\n",
    "    return t\n",
    "\n",
    "C_DIM = 10\n",
    "\n",
    "\n",
    "class EmbLayer:\n",
    "    def __init__(self):\n",
    "        # we place each character into C_DIM dimensional vector\n",
    "        self._emb = requires_grad(torch.randn(NUM_CLASSES, C_DIM))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        result = self._emb[inputs]\n",
    "        return result.view(inputs.shape[0], -1, C_DIM * 2)\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self._emb]\n",
    "\n",
    "    \n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, input_size, output_size, nonlinearity):\n",
    "        self._w = requires_grad(torch.randn(input_size, output_size))\n",
    "        self._b = requires_grad(torch.randn(output_size))\n",
    "        self._nonlinearity = nonlinearity\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        output = inputs @ self._w + self._b\n",
    "        return self._nonlinearity(output)\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self._w, self._b]\n",
    "\n",
    "\n",
    "class SqueezingLayer:\n",
    "    def forward(self, inputs):\n",
    "        return inputs.squeeze(dim=1)\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "\n",
    "class GroupingLayer:\n",
    "    def forward(self, inputs):\n",
    "        return inputs.view(inputs.shape[0], inputs.shape[1] // 2, inputs.shape[2] * 2)\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "\n",
    "def all_parameters(network):\n",
    "    parameters = []\n",
    "    for a_layer in network:\n",
    "        parameters.extend(a_layer.parameters())\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def softmax(logits, smooth=1e-4):\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdim=True) + smooth\n",
    "    return probs\n",
    "\n",
    "\n",
    "def train(inputs, outputs, num_iterations=100_000, verbose=False):\n",
    "    batch_size = 32\n",
    "    \n",
    "    assert len(inputs) == len(outputs)\n",
    "    hidden_output_size = 100\n",
    "    network = [\n",
    "        EmbLayer(),\n",
    "        # input = 8 characters\n",
    "        Layer(input_size=C_DIM * 2, output_size=hidden_output_size, nonlinearity=torch.tanh),\n",
    "        GroupingLayer(),\n",
    "        # input = 4 \"characters\"\n",
    "        Layer(input_size=hidden_output_size * 2, output_size=hidden_output_size, nonlinearity=torch.tanh),\n",
    "        GroupingLayer(),\n",
    "        # input = 2 \"characters\"\n",
    "        Layer(input_size=hidden_output_size * 2, output_size=hidden_output_size, nonlinearity=torch.tanh),\n",
    "        SqueezingLayer(),\n",
    "        # input = 1 \"character\"\n",
    "        Layer(input_size=hidden_output_size, output_size=NUM_CLASSES, nonlinearity=softmax),\n",
    "    ]\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        batch_indexes = torch.randint(0, inputs.shape[0], (batch_size,))\n",
    "        # inputs_batch.shape = (batch_size, CONTEXT_LEN)\n",
    "        inputs_batch = inputs[batch_indexes]\n",
    "        outputs_batch = outputs[batch_indexes]\n",
    "        probs = forward(inputs_batch, network)\n",
    "        m_probs = probs[torch.arange(len(probs)), outputs_batch]\n",
    "        loss = -(m_probs.log().mean())\n",
    "        loss.backward()\n",
    "        # learning_rate decay\n",
    "        learning_rate = 1e-1 if i < 100_000 else 1e-2\n",
    "        if verbose and i % 10000 == 0:\n",
    "            print(f'{time.time()=:.2f} {i=} {loss.data=}')\n",
    "            \n",
    "        for p in all_parameters(network):\n",
    "            p.data -= learning_rate * p.grad\n",
    "            p.grad = None\n",
    "    print(f'{loss=}')\n",
    "    return network\n",
    "\n",
    "\n",
    "def forward(inputs_batch, network):\n",
    "    outputs = inputs_batch\n",
    "    for a_layer in network:\n",
    "        outputs = a_layer.forward(outputs)\n",
    "    return outputs\n",
    "    \n",
    "\n",
    "TRAINING_SET_SIZE = 200_000\n",
    "NETWORK = train(\n",
    "    INPUTS[:TRAINING_SET_SIZE], \n",
    "    OUTPUTS[:TRAINING_SET_SIZE],\n",
    "    num_iterations=400_000, \n",
    "    verbose=True\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "57a18b15-31bc-45e1-b9f6-1d3d64150537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hasadi\n",
      "narya\n",
      "jylanna\n",
      "eretta\n",
      "zosine\n"
     ]
    }
   ],
   "source": [
    "def generate_examples(network):\n",
    "    num_examples = 5\n",
    "    for _ in range(num_examples):\n",
    "        \n",
    "        context = '.' * CONTEXT_LEN\n",
    "        example = ''\n",
    "        while True:\n",
    "            cur_inputs = torch.tensor([[CHAR_TO_CLASS[char] for char in context]])\n",
    "            probs = forward(cur_inputs, network)\n",
    "            next_class = torch.multinomial(probs, 1).item()\n",
    "            next_char = CLASS_TO_CHAR[next_class]\n",
    "            if next_char == '.':\n",
    "                break\n",
    "            example += next_char\n",
    "            context = context[1:] + next_char\n",
    "        print(example)\n",
    "\n",
    "generate_examples(NETWORK)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc1e565-8b84-4e60-8dfb-6bb6ea4b3eb3",
   "metadata": {},
   "source": [
    "Doesn't look like results from wavenet are much better than the results from mlp.\n",
    "Maybe that's because I didn't implement batchnorm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
