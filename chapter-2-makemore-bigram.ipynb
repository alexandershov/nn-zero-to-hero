{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57455a18-9953-4fb5-b4b8-35eb7e8c6ec8",
   "metadata": {},
   "source": [
    "In this chapter we build simple nn that uses the previous character to predict the next character.\n",
    "Output of nn are logits. Logits are logarithms of counts. When we do logits.exp() we get the counts.\n",
    "From counts we can get probabilities of getting given character class `(counts[i]/sum(counts))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd367d2d-0192-47b9-9145-61f22656b7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plot\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e54acb6-7864-4bd4-94f6-f00e3fc1d032",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_names(path):\n",
    "    names = []\n",
    "    with open(path) as fileobj:\n",
    "        for line in fileobj:\n",
    "            names.append(line.strip())\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02475f2d-ecad-4da7-bfbf-ae3144f2022e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32033, ['emma', 'olivia'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NAMES = read_names('names.txt')\n",
    "len(NAMES), NAMES[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "403f53ee-ef0f-40cf-a332-bf87b7a27aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(228146, [(0, 5), (5, 13), (13, 13)])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "import string\n",
    "\n",
    "STRING_TO_CLASS = {char: i for i, char in enumerate('.' + string.ascii_lowercase)}\n",
    "CLASS_TO_STRING = {i: char for char, i in STRING_TO_CLASS.items()}\n",
    "\n",
    "def build_training_set(names):\n",
    "    result = []\n",
    "    for a_name in names:\n",
    "        prev = '.'\n",
    "        for char in (a_name + '.'):\n",
    "            result.append((STRING_TO_CLASS[prev], STRING_TO_CLASS[char]))\n",
    "            prev = char\n",
    "    return result\n",
    "\n",
    "# TRAINING_SET consists of tuples prev_character -> next_character\n",
    "# characters are converted to integers using STRING_TO_CLASS\n",
    "\n",
    "TRAINING_SET = build_training_set(NAMES)\n",
    "len(TRAINING_SET), TRAINING_SET[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9832de66-61ec-4926-aabd-a9b7be1bdd71",
   "metadata": {},
   "source": [
    "`one_hot` is a simple encoding used in classification: it takes a number of classes\n",
    "and a tensor with class values.\n",
    "It returns a tensor where each class value is encoded in num_classes-dimensional tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fd131dd-b807-42b7-bebb-cf5bfeec9630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(torch.tensor([0, 3, 2]), num_classes=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3a3128d-e79c-4cc4-a12b-8a344a357d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0 loss.data=tensor(4.3815)\n",
      "i=100 loss.data=tensor(2.7272)\n",
      "i=200 loss.data=tensor(2.6082)\n",
      "i=300 loss.data=tensor(2.5617)\n",
      "i=400 loss.data=tensor(2.5364)\n",
      "i=500 loss.data=tensor(2.5203)\n",
      "i=600 loss.data=tensor(2.5092)\n",
      "i=700 loss.data=tensor(2.5012)\n",
      "i=800 loss.data=tensor(2.4950)\n",
      "i=900 loss.data=tensor(2.4902)\n",
      "i=1000 loss.data=tensor(2.4863)\n",
      "i=1100 loss.data=tensor(2.4831)\n",
      "i=1200 loss.data=tensor(2.4803)\n",
      "i=1300 loss.data=tensor(2.4780)\n",
      "i=1400 loss.data=tensor(2.4759)\n"
     ]
    }
   ],
   "source": [
    "def train(training_set):\n",
    "    inputs = torch.tensor([input_ for input_, _ in training_set])\n",
    "    expected_outputs =  torch.tensor([output for _, output in training_set])\n",
    "    num_classes = len(STRING_TO_CLASS)\n",
    "    inputs = F.one_hot(inputs, num_classes=num_classes).float()\n",
    "    expected_outputs = F.one_hot(expected_outputs, num_classes=num_classes).float()  # (n, 27)\n",
    "    w = torch.randn((num_classes, num_classes))\n",
    "    w.requires_grad = True\n",
    "    b = torch.randn(num_classes)\n",
    "    b.requires_grad = True\n",
    "    \n",
    "    num_iter = 1500\n",
    "    learning_rate = 2\n",
    "    smooth = 0.0001\n",
    "    for i in range(num_iter):\n",
    "        # we consider logits to be logarithms of counts\n",
    "        # if logit is negative, then count is close to 0\n",
    "        logits = (inputs @ w + b)\n",
    "        counts = (inputs @ w + b).exp()  # (n, 27) @ (27, 27) = (n, 27); (n, 27) + (27) = (n, 27)\n",
    "        # probs are probabilities now\n",
    "        prob = counts / counts.sum(dim=1, keepdim=True)\n",
    "        # negative log likelihood: we want to maximize product of probabilities\n",
    "        # when we take negative sign, then we want to minimize the resulting function\n",
    "        # it checks out: we want to minimize loss\n",
    "        probs = prob[expected_outputs == 1] + smooth\n",
    "        loss = -probs.log().mean()\n",
    "\n",
    "        loss.backward()\n",
    "        w.data -= learning_rate * w.grad\n",
    "        b.data -= learning_rate * b.grad\n",
    "        w.grad = None\n",
    "        b.grad = None\n",
    "        if i % 100 == 0:\n",
    "            print(f'{i=} {loss.data=}')\n",
    "    return w, b\n",
    "        \n",
    "    \n",
    "W, B = train(TRAINING_SET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b751773-a691-4881-90f2-bdbbc1d66eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpi\n",
      "jph\n",
      "elynelion\n",
      "joniyn\n",
      "ny\n"
     ]
    }
   ],
   "source": [
    "def generate_examples(w, b):\n",
    "    num_examples = 5\n",
    "    for _ in range(num_examples):\n",
    "        \n",
    "        cur_char = '.'\n",
    "        example = ''\n",
    "        while True:\n",
    "            cur_one_hot = F.one_hot(torch.tensor([STRING_TO_CLASS[cur_char]]), num_classes=len(STRING_TO_CLASS)).float()\n",
    "            output = (cur_one_hot @ w + b).exp()\n",
    "            probs = output / output.sum()\n",
    "            next_class = torch.multinomial(probs, 1).item()\n",
    "            next_char = CLASS_TO_STRING[next_class]\n",
    "            if next_char == '.':\n",
    "                break\n",
    "            example += next_char\n",
    "            cur_char = next_char\n",
    "        print(example)\n",
    "\n",
    "generate_examples(W, B)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89f6cba-9c28-4bed-9aaa-6c22acfd9b73",
   "metadata": {},
   "source": [
    "We can see that generated examples suck.\n",
    "\n",
    "That's because we only use information about the previous character to generate the next character.\n",
    "We can do better.\n",
    "See next chapter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
